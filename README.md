# Attacking ML Classifier with Adversarial Images 

One limitation to using convolutional neural networks (CNN) to classify images and objects is their ability to be fooled using adversarial images. Adversarial images are inputs into the CNN which apply a noise or small pertubation to the examples causing the ML classifier to misclassify the inputs. Adversarial images work by taking advantage of the linear nature  of neural networks and can be explained as the property of high-dimensional dot products. 
